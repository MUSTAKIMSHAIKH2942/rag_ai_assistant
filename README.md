# ğŸ¤– RAG AI Assistant (Free & Local)

A lightweight Retrieval-Augmented Generation (RAG) assistant built with **Streamlit, LangChain, FAISS, and Ollama (Mistral model)**. Designed for querying internal company documents (PDFs) with privacy â€” fully offline, free, and fast.

---

## ğŸ§  Step-by-Step Setup

### 1ï¸âƒ£ Install Ollama + Mistral LLM

> Ollama lets you run local LLMs like Mistral with ease.

- ğŸ”— Download Ollama: https://ollama.com/download  
- Pull the Mistral model:
  ```bash
  ollama pull mistral
2ï¸âƒ£ Install Python Dependencies
Ensure Python 3.10+ is installed.


pip install -r requirements.txt
3ï¸âƒ£ Add PDFs & Create Vector DB
Place your PDF files inside the data/company_docs/ folder.


python ingest.py
This script will:

Load PDF files

Split and embed the documents using all-MiniLM-L6-v2

Save the vector index using FAISS to vectordb/

4ï¸âƒ£ Launch the Streamlit UI
bash
Copy
Edit
streamlit run app.py
Youâ€™ll get a web interface to ask questions about the uploaded documents.

ğŸ’¬ Example Questions to Ask
â€œWhat is the process for leave approval?â€

â€œSummarize the company policy on remote work.â€

â€œWhat does the contract say about notice period?â€

â€œGive me bullet points from the vacation policy.â€

ğŸ› ï¸ Tech Stack (Free & Local)
Component	Tool / Library
Embedding Model	sentence-transformers (MiniLM)
Vector Database	FAISS (offline, fast)
LLM	Ollama + Mistral (open-source)
UI	Streamlit
RAG Framework	LangChain

ğŸš€ Features
ğŸ’¾ Offline document retrieval

ğŸ“„ Supports PDF ingestion

ğŸ” Semantic search

ğŸ§  Local LLM for privacy & speed

ğŸª¶ Lightweight and open-source

ğŸ“Œ Notes
Make sure Ollama is running before launching the app.

You can customize the LLM in rag_engine.py (e.g., try llama2, gemma, etc. with ollama pull llama2).

All document embeddings are stored in vectordb/.

